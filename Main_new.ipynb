{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn.metrics as metrics\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy import sparse\n",
    "from functools import reduce\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, cross_val_score, GroupKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    \n",
    "    titles_df = pd.read_csv('./data/docs_titles.tsv/docs_titles.tsv', sep='\\t')\n",
    "    docs_id_test = pd.read_csv('./data/test_groups.csv', sep=',')\n",
    "    docs_id_train = pd.read_csv('./data/train_groups.csv', sep=',')\n",
    "\n",
    "    info = pd.concat([docs_id_train, docs_id_test])\n",
    "    info.reset_index(drop=True)\n",
    "\n",
    "    titles = pd.merge(titles_df, info[['group_id', 'doc_id', 'target']], on='doc_id', how='inner')\n",
    "    titles['title'] += ' '\n",
    "    titles['title'].fillna(' ', inplace=True)\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles_extraction():\n",
    "    \n",
    "    titles = get_df();\n",
    "\n",
    "    titles = titles[['title', 'group_id']].groupby('group_id').sum()['title']\n",
    "\n",
    "    titles = titles.apply(lambda x: cleaner(x))\n",
    "    titles = titles.values\n",
    "\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(titles, group_num, bad_words):\n",
    "    titles = titles.lower()\n",
    "    titles = re.sub(r'\\W', '  ', titles)\n",
    "    \n",
    "    for i in bad_words[group_num]:\n",
    "            titles = titles.replace(i, ' ')\n",
    "    titles = re.sub(r'\\s+', ' ', titles)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стеммниг для русского языка (взял в интернете)\n",
    "class Porter:\n",
    "    PERFECTIVEGROUND =  re.compile(u\"((ив|ивши|ившись|ыв|ывши|ывшись)|((?<=[ая])(в|вши|вшись)))$\")\n",
    "    REFLEXIVE = re.compile(u\"(с[яь])$\")\n",
    "    ADJECTIVE = re.compile(u\"(ее|ие|ые|ое|ими|ыми|ей|ий|ый|ой|ем|им|ым|ом|его|ого|ему|ому|их|ых|ую|юю|ая|яя|ою|ею)$\")\n",
    "    PARTICIPLE = re.compile(u\"((ивш|ывш|ующ)|((?<=[ая])(ем|нн|вш|ющ|щ)))$\")\n",
    "    VERB = re.compile(u\"((ила|ыла|ена|ейте|уйте|ите|или|ыли|ей|уй|ил|ыл|им|ым|ен|ило|ыло|ено|ят|ует|уют|ит|ыт|ены|ить|ыть|ишь|ую|ю)|((?<=[ая])(ла|на|ете|йте|ли|й|л|ем|н|ло|но|ет|ют|ны|ть|ешь|нно)))$\")\n",
    "    NOUN = re.compile(u\"(а|ев|ов|ие|ье|е|иями|ями|ами|еи|ии|и|ией|ей|ой|ий|й|иям|ям|ием|ем|ам|ом|о|у|ах|иях|ях|ы|ь|ию|ью|ю|ия|ья|я)$\")\n",
    "    RVRE = re.compile(u\"^(.*?[аеиоуыэюя])(.*)$\")\n",
    "    DERIVATIONAL = re.compile(u\".*[^аеиоуыэюя]+[аеиоуыэюя].*ость?$\")\n",
    "    DER = re.compile(u\"ость?$\")\n",
    "    SUPERLATIVE = re.compile(u\"(ейше|ейш)$\")\n",
    "    I = re.compile(u\"и$\")\n",
    "    P = re.compile(u\"ь$\")\n",
    "    NN = re.compile(u\"нн$\")\n",
    "\n",
    "    def stem(string):\n",
    "        \n",
    "        changed = ''\n",
    "        \n",
    "        string = string.lower()\n",
    "\n",
    "        for word in string.split():\n",
    "            if not word.isdigit():\n",
    "                word = word.replace(u'ё', u'е')\n",
    "                m = re.match(Porter.RVRE, word)\n",
    "\n",
    "                if m and m.groups():\n",
    "                    pre = m.group(1)\n",
    "                    rv = m.group(2)\n",
    "                    temp = Porter.PERFECTIVEGROUND.sub('', rv, 1)\n",
    "                    if temp == rv:\n",
    "                        rv = Porter.REFLEXIVE.sub('', rv, 1)\n",
    "                        temp = Porter.ADJECTIVE.sub('', rv, 1)\n",
    "                        if temp != rv:\n",
    "                            rv = temp\n",
    "                            rv = Porter.PARTICIPLE.sub('', rv, 1)\n",
    "                        else:\n",
    "                            temp = Porter.VERB.sub('', rv, 1)\n",
    "                            if temp == rv:\n",
    "                                rv = Porter.NOUN.sub('', rv, 1)\n",
    "                            else:\n",
    "                                rv = temp\n",
    "                    else:\n",
    "                        rv = temp\n",
    "\n",
    "                    rv = Porter.I.sub('', rv, 1)\n",
    "\n",
    "                    if re.match(Porter.DERIVATIONAL, rv):\n",
    "                        rv = Porter.DER.sub('', rv, 1)\n",
    "\n",
    "                    temp = Porter.P.sub('', rv, 1)\n",
    "                    if temp == rv:\n",
    "                        rv = Porter.SUPERLATIVE.sub('', rv, 1)\n",
    "                        rv = Porter.NN.sub(u'н', rv, 1)\n",
    "                    else:\n",
    "                        rv = temp\n",
    "                    word = pre+rv\n",
    "\n",
    "            changed += word + ' '\n",
    "\n",
    "        return changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # эта и 3 следующие функции используются для подсчета расстояний без тф идф и косинусной метрики\n",
    "\n",
    "# def features_create(mode, groups_titledata):\n",
    "    \n",
    "#     if mode == 'train':\n",
    "#         y = []      \n",
    "#     X = []\n",
    "#     groups_train = []\n",
    "    \n",
    "#     for new_group in groups_titledata:\n",
    "#         docs = groups_titledata[new_group]\n",
    "        \n",
    "#         for k, info in enumerate(docs):\n",
    "            \n",
    "#             doc_id = info[0]\n",
    "#             title = info[1]\n",
    "            \n",
    "#             if mode == 'train':\n",
    "#                 target_id = info[2]\n",
    "#                 y.append(target_id)\n",
    "                \n",
    "#             groups_train.append(new_group)\n",
    "#             all_dist = []\n",
    "#             words = set(title.strip().split())\n",
    "            \n",
    "#             for j in range(0, len(docs)):\n",
    "#                 if k == j:\n",
    "#                     continue\n",
    "#                 info = docs[j]\n",
    "#                 doc_id_j = info[0]\n",
    "#                 title_j = info[1]\n",
    "\n",
    "#                 words_j = set(title_j.strip().split())\n",
    "#                 all_dist.append(len(words.intersection(words_j)))\n",
    "                \n",
    "#             X.append(sorted(all_dist, reverse=True)[0:25])\n",
    "            \n",
    "#     X = np.array(X)\n",
    "    \n",
    "#     if mode == 'train':\n",
    "#         y = np.array(y)\n",
    "    \n",
    "#     groups_train = np.array(groups_train)\n",
    "\n",
    "#     if mode == 'train':\n",
    "#         print(X.shape, y.shape, groups_train.shape)\n",
    "#         return X, y, groups_train\n",
    "#     else:\n",
    "#         print(X.shape, groups_train.shape)\n",
    "#         return X, groups_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tuple_x(a1, a2):\n",
    "#     return a1, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def title_info_dict(mode, doc_to_title):\n",
    "    \n",
    "#     data = pd.read_csv('./data/{}_groups.csv'.format(mode))\n",
    "\n",
    "#     titledata = {}\n",
    "    \n",
    "#     for i in range(len(data)):\n",
    "        \n",
    "#         new_doc = data.iloc[i]\n",
    "#         doc_group = new_doc['group_id']\n",
    "#         doc_id = new_doc['doc_id']\n",
    "            \n",
    "#         title = doc_to_title[doc_id]\n",
    "        \n",
    "#         if doc_group not in titledata:\n",
    "#             titledata[doc_group] = []\n",
    "            \n",
    "#         if mode == 'train':\n",
    "#             titledata[doc_group].append((doc_id, title, new_doc['target']))\n",
    "#         else:\n",
    "#             titledata[doc_group].append((doc_id, title))\n",
    "        \n",
    "#     return titledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def easy_launch():\n",
    "    \n",
    "#     doc_to_title = {}\n",
    "#     with open('./data/unversal_table.csv', encoding = 'utf-8') as f:\n",
    "#         for num_line, line in enumerate(f):\n",
    "#             if num_line == 0:\n",
    "#                 continue\n",
    "\n",
    "#             line = line.replace('\\t', ',')\n",
    "#             data = line.strip().split(',')\n",
    "\n",
    "#             doc_id = int(data[0])\n",
    "#             if len(data) == 1:\n",
    "#                 title = ''\n",
    "#             else:\n",
    "#                 title = data[1]\n",
    "#             doc_to_title[doc_id] = title\n",
    "            \n",
    "#     print('doc titles dict len = {}'.format(len(doc_to_title)))\n",
    "    \n",
    "#     train_titledata = title_info_dict('train', doc_to_title)\n",
    "#     test_titledata = title_info_dict('test', doc_to_title)\n",
    "    \n",
    "#     X_train, y_train, groups_train = features_create('train', train_titledata)\n",
    "#     X_test, groups_test = features_create('test', test_titledata)\n",
    "    \n",
    "#     return X_train, y_train, X_test, groups_train, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3  функции для перебора комбинаций параметров\n",
    "def flatten(x):\n",
    "    \n",
    "    result = []\n",
    "    for elem in x:\n",
    "        if hasattr(elem, \"__iter__\") and not isinstance(elem, str):\n",
    "            result.extend(flatten(elem))\n",
    "        else:\n",
    "            result.append(elem)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_concat(list1, list2):\n",
    "    \n",
    "    len1 = len(list1)\n",
    "    len2 = len(list2)\n",
    "    \n",
    "    return [[list1[i], list2[j]] for i in range(len1) for j in range(len2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(params):\n",
    "    \n",
    "    list_ = []\n",
    "    for value in params.values():\n",
    "        list_.append(value)\n",
    "\n",
    "    tmp = reduce(lambda x, y: list_concat(x, y), list_)\n",
    "\n",
    "    res = []\n",
    "    for elem in tmp:\n",
    "        param_list = flatten(elem)\n",
    "        param_dict = dict(zip(params.keys(), param_list))\n",
    "        res.append(param_dict)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(X_train, train_target, model, params, folds_gen_func, folds_num=10, thresholds=[0.32], **kwargs):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    main_res = []\n",
    "    for param_set in combinations(params):\n",
    "        \n",
    "        print(param_set)        \n",
    "        exact_model = model(**param_set) \n",
    "        \n",
    "        fold_generator = folds_gen_func(folds_num)\n",
    "        \n",
    "        for th in thresholds:\n",
    "#           th = 0.27\n",
    "            print('th = ', th)\n",
    "            res = []\n",
    "            for train_index, test_index in fold_generator.split(X_train, train_target, **kwargs):\n",
    "\n",
    "                exact_model.fit(scaler.transform(X_train[train_index]), train_target[train_index])\n",
    "\n",
    "                y_pred = [0 if val < th else 1 for val in exact_model.predict_proba(scaler.transform(X_train[test_index]))[:,1]]\n",
    "\n",
    "                score = metrics.f1_score(train_target[test_index],\\\n",
    "                                                y_pred)\n",
    "    #               print('threshold = {}, score = {}'.format(th, score))\n",
    "\n",
    "                res.append(score)\n",
    "        #                                       exact_model.predict(scaler.transform(X_train[test_index]))))\n",
    "\n",
    "\n",
    "            mean = sum(res)/len(res)\n",
    "            print(mean)\n",
    "#             print('threshold = {}, score = {}'.format(th, mean))\n",
    "            main_res.append((mean, param_set, th))\n",
    "    \n",
    "    best = main_res[np.argmax([res[0] for res in main_res])]\n",
    "    print('--------max-------')\n",
    "    print(best)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняет решение\n",
    "\n",
    "def save_submission(y_pred):\n",
    "\n",
    "    data = pd.read_csv('data/test_groups.csv')\n",
    "    print('len data = ', len(data))\n",
    "    data['target'] = y_pred\n",
    "    \n",
    "    data = data.drop(['group_id', 'doc_id'], axis=1)\n",
    "\n",
    "    data.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    info = np.unique(data['target'], return_counts=True)\n",
    "    \n",
    "    if info[0].shape[0] > 1:\n",
    "        \n",
    "        print('0: {}, 1: {}'.format(info[1][0], info[1][1]))\n",
    "        if info[1][1] > 6000 or info[1][1] < 2500:\n",
    "            print('Your submisson is shit')\n",
    "#         elif info[1][1] > 4500:\n",
    "#             print('Your submisson is probably shit')\n",
    "    else:\n",
    "        print('There are only {} in submission'.format(info[0][0]))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, X_test, train_target, model, scaler=None, **kwargs):\n",
    "    \n",
    "    curr_model = model(**kwargs)\n",
    "    \n",
    "    if scaler is not None:\n",
    "        \n",
    "        your_scaler = scaler()\n",
    "        your_scaler.fit(X_train)\n",
    "        X_train = your_scaler.transform(X_train)\n",
    "        X_test = your_scaler.transform(X_test)\n",
    "        \n",
    "    curr_model.fit(X_train, train_target)\n",
    "#     y_pred = curr_model.predict(X_test)\n",
    "    th = 0.35\n",
    "    y_pred = [0 if val < th else 1 for val in curr_model.predict_proba(X_test)[:,1]]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лемматизация\n",
    "\n",
    "def str_parser(words_):\n",
    "    global j\n",
    "    new_string = ''\n",
    "    \n",
    "    for i in re.findall(r'\\b[а-я]{1,20}\\b', words_):\n",
    "        new_string += (morph.parse(i)[0].normal_form) + ' '\n",
    "\n",
    "    j += 1\n",
    "    if(j % 100 == 0):\n",
    "        print(j, '/28026 loaded')\n",
    "    \n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    \n",
    "    return df['title'].apply(lambda x: str_parser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_titles(df):\n",
    "    stemming = Porter\n",
    "    return df['title'].apply(lambda x: stemming.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_():\n",
    "\n",
    "    df_train = pd.read_csv('data/core_train.csv')\n",
    "    df_test = pd.read_csv('data/core_test.csv')\n",
    "    df_train.fillna(' ', inplace=True)\n",
    "    df_test.fillna(' ', inplace=True)\n",
    "    titles_df = pd.concat([df_train, df_test],ignore_index = True)\n",
    "    titles_df.drop(columns = {'Unnamed: 0'}, inplace = True)\n",
    "\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчитывает расстояния между документами берет 20 наименьших и сохраняет numpy ndarray в файл\n",
    "\n",
    "def features_save(group_num, max_f=None, vec_type=1):\n",
    "    \n",
    "    df = pd.read_csv('./data/unversal_table.csv')\n",
    "    df.fillna(' ', inplace=True)\n",
    "    corpus = df[df.group_id == group_num]['title'].values\n",
    "#     print(corpus)\n",
    "    if vec_type == 1:\n",
    "        vectorizer = CountVectorizer(max_features=max_f)\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "    elif vec_type ==2:\n",
    "        vectorizer2 = CountVectorizer(max_features=max_f)\n",
    "        X = vectorizer2.fit_transform(corpus)\n",
    "#     print(vectorizer.vocabulary_)\n",
    "    features = count_distances(X.toarray())\n",
    "#     print(features.shape)\n",
    "    np.save('group_features/{}'.format(group_num), features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчет расстояний по матрице встречаемости (косинусная метрика)\n",
    "\n",
    "def count_distances(docs):\n",
    "    \n",
    "    distance = cdist(docs, docs, 'cosine')\n",
    "    \n",
    "    res = np.asarray([np.concatenate((vec[:num],vec[num+1:])) for num, vec in enumerate(distance)])\n",
    "    res = np.sort(res)[:, :25]\n",
    "#     res = np.flip(res, axis=1)[:, :25]\n",
    "#     info = 26 - res.shape[1]\n",
    "#     if info > 0:\n",
    "#         z = np.zeros(shape=(res.shape[0], info))\n",
    "#         res = np.hstack((z, res))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проходит по заданным группам и сохраняет признаки(расстояния)\n",
    "\n",
    "def main_parser_and_saver(start_group, finish_group):\n",
    "    \n",
    "    for group_num in range(start_group, finish_group + 1):\n",
    "\n",
    "        features_save(group_num)\n",
    "        print('Скачалась группа:', group_num)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создает X_train, X_test, train_target\n",
    "\n",
    "def prepare_data():\n",
    "    \n",
    "    X_train = all_group_feature_list(1, 129)\n",
    "    X_test = all_group_feature_list(130, 309)\n",
    "    \n",
    "    d = pd.read_csv('./data/train_groups.csv')\n",
    "    train_target = d['target']\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0, posinf=1, neginf=0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0, posinf=1, neginf=0)\n",
    "    \n",
    "    return X_train, train_target, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружает файлы с признаками документов по группам\n",
    "\n",
    "def all_group_feature_list(start_group, finish_group):\n",
    "    \n",
    "    res = np.load('group_features/{}.npy'.format(start_group))\n",
    "  \n",
    "    for group_num in range(start_group + 1, finish_group + 1):\n",
    "#         res += ndarray_to_list(np.load('group_features/{}.npy'.format(group_num)))\n",
    "        \n",
    "        t = np.load('group_features/{}.npy'.format(group_num))\n",
    "#         print(res.shape, t.shape, group_num)\n",
    "        res = np.vstack((res, t))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = get_train_test_()\n",
    "titles = pd.DataFrame({'doc_id': titles_df['doc_id'] ,\n",
    "                       'title': titles_df['title'] + titles_df['h1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 10 µs, total: 14 µs\n",
      "Wall time: 35.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j = 0\n",
    "# titles['title'] = lemmatization(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.01 s, sys: 151 ms, total: 8.16 s\n",
      "Wall time: 8.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "titles['title'] = stemming_titles(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.to_csv('./data/unversal_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc titles dict len = 28026\n",
      "(11690, 25) (11690,) (11690,)\n",
      "(16627, 25) (16627,)\n",
      "CPU times: user 12.7 s, sys: 45.4 ms, total: 12.7 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, y_train, X_test, groups_train, groups_test = easy_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_train = pd.read_csv('data/train_groups.csv')['group_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06, 'n_estimators': 150}\n",
      "th =  0.22\n",
      "0.6905247109429726\n",
      "th =  0.23\n",
      "0.6918590992745755\n",
      "th =  0.24\n",
      "0.6950857871928419\n",
      "th =  0.25\n",
      "0.6973429128734442\n",
      "th =  0.26\n",
      "0.7002258476889112\n",
      "th =  0.27\n",
      "0.7020797215310617\n",
      "--------max-------\n",
      "(0.7020797215310617, {'learning_rate': 0.06, 'n_estimators': 150}, 0.27)\n",
      "CPU times: user 5min 6s, sys: 691 ms, total: 5min 7s\n",
      "Wall time: 5min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = validation(X_train, y_train, GradientBoostingClassifier, params, GroupKFold, groups=groups_train, thresholds=th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len data =  16627\n",
      "0: 11334, 1: 5293\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_train, X_test, y_train, GradientBoostingClassifier, StandardScaler, **best[1])\n",
    "data = save_submission(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': [0.07, 0.08],\n",
    "#           'n_estimators': [372, 374]}\n",
    "          'n_estimators': [100, 150, 200, 250]}\n",
    "th = [0.26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': [0.06],\n",
    "          'n_estimators' : [150]}\n",
    "th = [0.22, 0.23, 0.24, 0.25, 0.26, 0.27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'algorithm': ['ball_tree', 'brute'],\n",
    "# params = {'algorithm': ['ball_tree'],\n",
    "# params = {'algorithm': ['auto'],\n",
    "          'leaf_size': [5, 10, 20, 40],\n",
    "          'n_neighbors': [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80],\n",
    "#           'weights': [smart_weights],\n",
    "          'p': [1],\n",
    "          'n_jobs': [-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'algorithm': ['brute'],\n",
    "# params = {'algorithm': ['ball_tree'],\n",
    "# params = {'algorithm': ['auto'],\n",
    "          'n_neighbors': [60],\n",
    "#           'weights': [smart_weights],\n",
    "          'p': [1],\n",
    "          'n_jobs': [-1]}\n",
    "th = [0.3,0.31,0.32,0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = [0.32] \n",
    "params = {'n_estimators': [300, 400, 500, 600], \n",
    "          'criterion': ['gini', 'entropy'], \n",
    "          'max_depth': [10, 15, None], \n",
    "          'n_jobs': [-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': [1300], \n",
    "          'criterion': ['entropy'], \n",
    "          'max_depth': [10], \n",
    "          'n_jobs': [-1]}\n",
    "# th = [0.3,0.31,0.32,0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1300, 'criterion': 'entropy', 'max_depth': 10, 'n_jobs': -1}\n",
      "th =  0.32\n",
      "0.7072713780196176\n",
      "--------max-------\n",
      "(0.7072713780196176, {'n_estimators': 1300, 'criterion': 'entropy', 'max_depth': 10, 'n_jobs': -1}, 0.32)\n"
     ]
    }
   ],
   "source": [
    "best = validation(X_train, y_train, RandomForestClassifier, params, GroupKFold, groups=groups_train, thresholds=th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06, 'n_estimators': 150}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-5d6542394ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-285-3d8750c4d856>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(X_train, train_target, model, params, folds_gen_func, groups_num, thresholds, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mexact_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfold_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds_gen_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sphere-py37/lib/python3.7/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params, n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mleaf_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mmetric_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             n_jobs=n_jobs, **kwargs)\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "best = validation(X_train, y_train, KNeighborsClassifier, params, GroupKFold, groups=groups_train, thresholds=th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main_parser_and_saver(1, 129)\n",
    "# main_parser_and_saver(130, 309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/core_train.csv')\n",
    "df_test = pd.read_csv('data/core_test.csv')\n",
    "df_train.fillna(' ', inplace=True)\n",
    "df_test.fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titles_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_h2h3a = pd.read_csv('./data/no_bad_words_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = pd.merge(titles_all, titles_h2h3a[['doc_id', 'title']], on='doc_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df2 = pd.DataFrame({'doc_id': titles_df['doc_id'] ,\n",
    "                       'title': titles_df['title_x'] + titles_df['h1'],\n",
    "#                        'title': titles_df['title'] + titles_df['h1'],\n",
    "#                        'title': titles_df['h2'] + titles_df['h3'] + titles_df['a'],\n",
    "                       'group_id': titles_df['group_id']})\n",
    "\n",
    "titles_df2['title'] = stemming_titles(titles_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = pd.DataFrame({'doc_id': titles_df2['doc_id'] ,\n",
    "                       'title': titles_df2['title'] + titles_df['title_y'],\n",
    "#                        'title': titles_df['title'] + titles_df['h1'],\n",
    "#                        'title': titles_df['h2'] + titles_df['h3'] + titles_df['a'],\n",
    "                       'group_id': titles_df2['group_id']})\n",
    "\n",
    "titles.fillna(' ', inplace=True)\n",
    "j = 0\n",
    "# titles['title'] = stemming_titles(titles)\n",
    "# titles['title'] = lemmatization(titles)\n",
    "\n",
    "# titles['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ваз зам подшипник ступиц нив зам подшипник ступиц авт автозапчаст амортизатор ваз вал впрысков газ двигател диск задн зам карда колес коробк куз магазин мост неисправн нив общ опор охлажден передач передн подвеск подшипник привод пружин раздаточн регулировк руководств рулев рычаг сайт салон систем снят стабилизатор ступиц схем сцеплен тормозн трансмисс тюнинг управлен установк шаров шин электрооборудован '"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.to_csv('./data/unversal_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = prepare_data()\n",
    "groups_train = pd.read_csv('data/train_groups.csv')['group_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.06458565, 0.16333997, ..., 0.62203553, 0.65811827,\n",
       "        0.66563307],\n",
       "       [0.44056907, 0.44098301, 0.5       , ..., 0.77777778, 0.78918149,\n",
       "        0.78918149],\n",
       "       [0.48823368, 0.53343053, 0.53343053, ..., 0.75256417, 0.75686773,\n",
       "        0.80712081],\n",
       "       ...,\n",
       "       [0.88661066, 0.89793793, 0.90715233, ..., 0.97817821, 0.97867993,\n",
       "        0.97867993],\n",
       "       [0.3238766 , 0.78178211, 0.81101776, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.88529213, 0.90634142, 0.93082855, ..., 1.        , 1.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
